---
title: "Full Report Assignment Common Questions and Answers"
output: 
  html_document:
    toc: true
    toc_float:
      collapsed: true
    toc_depth: 3
    number_sections: false
    theme: "readable"
    highlight: "espresso"
---

<style>
h1:not(.title), #TOC > ul.tocify-header > li:first-child {
  font-weight: bold; 
  color: green; 
}

h2, #TOC ul.tocify-subheader[data-tag="2"] > li.tocify-item {
  color: blue; 
}

h3, #TOC ul.tocify-subheader[data-tag="3"] > li.tocify-item {
  color: purple;
  font-variant: small-caps; 
}

body{
  font-family: Arial;
}

</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# General Questions

### Is there one way to write a report?

No. A lot of materials on writing reports will tell you to do this or to do that. In the vast majority of cases this advice will be nothing more than that, advice, not hard and fast rules. That said, that advice is often given based on what makes a better reading experience for your intended audience and so is worth paying attention to. There will of course be some rules, on aspects such as presentation of information, values, citations, etc and, given that we use APA 7th edition, it is worth referring to this webpage and the instructional aids there for pointers on that aspect: [https://apastyle.apa.org/style-grammar-guidelines](https://apastyle.apa.org/style-grammar-guidelines){target="_blank"}

### What is the best way to write a report?

Again no real one answer but one key thing to keep in mind is your reader. Often we fall into the trap of thinking as the writer, because we are writing the report, and forget about the reader. So one good bit of advice is to write like a writer and read like a reader. This means that when you are redrafting your work, take a perspective of a reader who knows about psychology but perhaps does not know this topic so well. This will help you spot where ideas and information are not as clear as they could be.

### When is the deadline again and what are the marking criteria?

Best to check the Assessment Information Sheet on Moodle. We believe it is better to have one document that has correct information rather than multiple documents having conflicting information so we will keep that answer to the AIS.

### Is there a recommended word count for each part of the report?

So in terms of word count, whilst there is no specific hard and fast for each section a good ballpark is something like a third, a third, a third. Third on intro, third on discussion, third on the methods, results, abstract.  Don't take that verbatim though and just as a ballpark. We would  suggest though that the intro and discussion should balance (no very long intro and very small discussion, or vice versa, and any words we can save from the middle section of the methods and results, we put into the intro and discusison. Methods and Results are formulaic and detailed, they dont need a lot of expansion - so if you can get the detail across in less words then those spare words we would put into the intro and discussion. The intro and discussion of a paper do a lot of the heavy lifting basically.

### Do we need to include an Abstract?

Yes. Estimate somewhere between 250-400 for this but in reality most abstracts will be around 250 words and that allows more words for other parts. Main thing is that it covers the key parts of Area, Aim, Methods, Results, Conclusion.


### What is included in the word count?

An easier way is to say what is not included in the word count. The title, the references, and the appendices (if you have any) are the only things not included in the word count. Everything else - including figure legends, table legends, citations, etc, and all the text from the abstract to the end of the discussion, are included in the word count

### Can we use abbreviations?

Abbreviations are really not great for readers - they are only good for writers in that it helps them save space. For a reader it creates a high cognitive demand as they try to remember what the abbreviation stood for, ultimately distracting them from reading and understanding what you are writing - an example here might be using SE for Self-Efficacy and IM for Intrinsic Motivation. Taking this approach makes the writing very hard to understand and we would discourage it at all times. The exception here is questionnaires (e.g. MSLQ) and well-known techniques or methods (e.g. MRI, MEG, EEG) but remember to state the phrase out in full in the first instance.

### Can we use quotations from previous work?

Technically this is allowed but would be better if you didn't and just rephrased the writing in your own words. For one, quotations change the tone of the writing from your tone to that of the other writer. More importantly, when you use a quotation, and don't go into depth about the meaning of that quotation, you are relying on the reader getting the exact same meaning from the quotation that you did but they are only getting the quotation out of context. A much better approach then is to restate the meaning so that it fits and flows with the surrounding context of your writing.

### Can we use do not or don't?

Do not use don't and other such shortenings like can't and shouldn't. Instead use do not etc. Remember academic tone!

### Can we use footnotes?

We would say no. They are not the done thing in academic reports in Psychology so best avoided. They are not conducive to an easy reading experience of the reader - they have to move their eyes from where they were reading to the bottom of the page to find the footnote to back to where they were in the page. Best avoided.

### Can you use subheadings in an Introduction and a Discussion?

Yes. There is no rule against this. The question is really do you need them. Often when people use subheadings in an introduction and discussion it leads them to write separate sections in the introduction and the different parts are not connected or joined in any way and it reads like different reports in the one introduction. If you do use subheadings try to think about your writing to still connect the different sections and show they are interlinked in some fashion. Even a simple sentence like "A second important question related to Self-Efficacy is..." This at least shows some brief acknowledgement of the first section and that everything is tied to self-efficacy and questions around it. Do remember though that subheadings count in the word count and if your sections are interlinked then you probably don't need them.

### What is Critical Evaluation?

This paragraph below has had all the points of critical evaluation highlighted in bold.

In regard to perceived trustworthiness, **as with dominance in females, few** studies have specifically targeted the effect of vocal pitch on this trait, **with results proving largely inconsistent.** In male voices, two studies exploring the effect of vocal pitch on election voting and personality traits **found contrasting results;** Tigue et al. (2012) found that low-pitched candidates were perceived as more trustworthy, **whereas** Klofstad et al. (2012) found no significant preference in pitch with regard to trustworthiness. **Similarly,** exploring the effect of seeking short- versus long-term relationships, Vukovic et al. (2011) showed no significant preference in male pitch when female participants judged the voices for trustworthiness. **However,** McAleer et al. (2014) found that high-pitched male voices were rated as more trustworthy compared with low-pitched voices. Considering trustworthiness in female voices, **as is the case with dominance, there is a lack of studies, and again a lack of consistent findings.** Klofstad et al. (2012) found that low-pitched female speakers were judged as being more trustworthy in a political scenario, **while, in contrast,** McAleer et al. (2014) found that trustworthiness in female voices was not influenced by average voice pitch but by the vocal glide and intonation; how the pitch moves. **In short, in both genders, there is a lack of studies examining the role of pitch and the preference of high versus low pitch in trustworthiness judgments, with previous findings having failed to provide a consistent answer.**

Hopefully what you can see is that the critical evaluation parts are turning the paragraph from a purely descriptive list of findings into a discussion about how previous findings relate to each other (how they compare, align, contrast), and how they all come together in your report to help build the arguement/rationale for your work (or the rationale for your theory). This paragraph comes from [Tsantani et al., 2016](https://journals.sagepub.com/doi/10.1177/0301006616643675){target="_blank"} and further discussion about this paragraph can be seen on the TEAMS channel by searching "Writing Paragraphs" in a thread started by Phil on the 25th May @ 9:59. Our thanks to Eniko Dobos for help in creating this example.

### Do you have any guidance for writing paragraphs?

This blog is one that we quite like: [How to write paragraphs](https://blogs.lse.ac.uk/writingforresearch/2017/07/17/how-to-write-paragraphs-in-research-texts-articles-books-and-phds/){target="_blank"}. We do not agree with everything it says, in particular point 5 that a paragraph cant be more than 250 words, but a lot of the other points to watch out for are really helpful, and the general overall principle of the structure of a paragraph is helpful as well - you may also know it as Point, Evidence, Explain, Link. Further discussion about this paragraph can be seen on the TEAMS channel by searching "Writing Paragraphs" in a thread started by Phil on the 25th May @ 9:59.


# Using the Pre-Registration

### Should we include the pre-registration in the submission?

This is not essential but normally we ask that you include your answers to the first 5 questions from the pre-registration as an appendix to the full report. Do not worry if you forget this though as it is not essential and just recommended. You can include the code as well if that is easier but again not essential. Having the pre-registration as an appendix helps the reader check things if they are uncertain.

### Can we refer to the pre-registration in the appendix?

Yes but this must not be used as a means to save words. For example, you can say "We excluded participants based on the criteria of under 21, male and mature student (see Appendix A)" but you can't say "We excluded participants based on criteria (see Appendix A)". The report has to stand alone and as such relevant information should be contained in the full report.

### Can we use points, citations, research raised in the pre-registration for the full report?

Absolutely. You should definitely use the same research and citations. You can also make the same points though just watch the wording and trying to make sure that the phrasing is in your own tone and in your own words. In short, the pre-registration is a snapshot of what you are doing. The full report is a deeper expansion on those ideas.

### Can we or should we compare the results from our full dataset to the results from the pre-registration?

No! Really no! The pre-registration is just a template to help you think about what you want to do with your data. The results from the pre-registration should not be talked about in the full report. 

### Can I change my pre-registration to avoid having to write about a deviation from the pre-registration?

The real question is "should you change your pre-registration?" The answer to that would be no, we would probably suggest just leaving the pre-registration as is and writing about the deviation briefly as shown in the videos. To do good and honest science involves having to write up some issues we have. That is fine!

### I have now done a lot of reading and think my pre-registration is completely wrong and want to change it all. Can I?

Ultimately we will grade and give feedback on the full report you submit. It is unadvised to change the pre-registration because you won't have any feedback on it and your grade won't change for it, but ultimately we will grade and give feedback on the full report you submit and not on the pre-registration.

# Introductions & Discussions

### What is the best way to structure our introductions?

The most common advice is to run broad to narrow and that holds here as well. The tricky part here is of course that you are working with probably three variables and trying to bring that altogether. Obviously there are a million different ways to bring all this information together but if it was us we would default to the notion of no, matter what we do, the hypotheses are always in the last paragraph of the introduction. That may mean that one research question, and the information leading to it, is half-way up the intro and the other Research Question is much further down the intro. Or it might mean that both RQs and both hypotheses are near the end, but yeah the intro to the methods transition works best when the hypotheses are at the end of the intro. There is of course no issue in restating one hypothesis twice (midway and at the end) but that might be a bit heavy on words. All that said, there is nothing wrong per se of having one hypothesis halfway up the intro - like it isn't illegal in any way - it just makes more logical sense to a reader in the methods if they are at the end of the introduction as this leads nicely into the methods and results.

### Should we give equal weight to all the variables in the introduction?

That is one option and that can work if you are equally interested in all variables and each are as equally important to what you are overall trying to show. Alternatively,  it might help to see the research as being focused on one variable (say Self-Efficacy) and having two questions about that one variable. One of the question taps into its relationship with another variable (test-anxiety), and the other question taps into the difference between two groups on that variable (maturity). Then your balance is more about giving equal weight to test-anxiety section and the maturity section, rather than trying to balance all three variables, as the self-efficacy variable runs through both sections. This might give a slightly clunky transition between the two sections but that is ok here. 

### Can I change my hypothesis?

In practice, what you would probably do here if running your own full project would be depending on whether you had run the analysis or not. As soon as you run the analysis you should always write up what you originally intended and just deal with any differences in the discussion. If you haven't run the analysis you can effectively scrap everything and run a new pre-reg. But yeah, the point of no return, if you like, is running the analysis.

### Can I say my study is a replication?

There is a distinction between direct and conceptual replication. With direct replication you are pretty much doing the exact same test as the original paper. With a conceptual replication it is a bit vaguer - that would be more close to such and such was found to hold in X, and if that reasoning and theory is correct then the same effect should be seen in Y.  So that can be stated. However, when a direct replication becomes a conceptual replication, and when a conceptual replication becomes just a whole new study inspired by something previous is really not clearly defined. Long story short, here you might think of this work as a conceptual replication but it might not be a direct replication.

### Should the full report have the same hypothesis as the pre-registration had?

If the question is should the full report have the same hypotheses as was stated in the pre-registration then the answer would be most likely. You can deviate or change of course but be sure to do that before you look at the data and definitely not after. At this stage it can be better to just go with hypotheses you had though and deal with differences in the Discussion.

### Is it an issue if the results are opposite to the hypothesis we stated - e.g. we hypothesised a negative correlation but it turned out to be positive?

This is not an issue and can make for a really interesting discussion. You have found something different from previous studies, assuming you based the hypothesis on previous studies, so you can now think about and discuss why that difference exists. Avoid assuming it is because you have done something wrong or the study was flawed - look for differences that might potentially explain the difference and perhaps build future directions from that. That said, if there were limitations that may have raised an issue then perhaps discuss them as well.

### Can we change the rationale for our effect size or other aspects of what we have proposed? And how might we do this without sounding clunky?

Again this assumes you haven't run the analysis first but there is nothing wrong in saying “Originally we had planned for ….. based on research into……..(citation). However, on reflection, and based on ………(citation) it would be more appropriate to estimate an effect size…….” That is totally fine and totally honest and would be something we would write. Really not about trying to catch people out in research and more just asking people to state what they are doing and why. However, if you have run the analysis then just deal with any issues and differences in the Discussion and don't go back and change part of the analysis to get a better/different finding.

### My results weren't significant, am I going to fail?

No. This is never about finding a significant result. It is purely about writing up research regardless of the outcome.

### Do we need to stick to a rigid Discussion format of summarise all findings first then theory?

Just to start with, the full question was "on the structure of our Discussion sections. Given that we have two studies/hypotheses, would it be preferable to first summarise our findings for our first study/hypothesis and relate them to previous findings and theory and then move on to do the same for our second study/hypothesis? Or do we need to stick to a format where we first summarise our findings for both hypotheses/studies and then relate the findings for both our studies to previous findings and theory?" In reality, the common approach would summarise all findings first and then deal with each in turn but that doesnt mean what is suggested cant work. We think it might save words by taking the standard approach of all findings summarised and then each in turn related to theory and previous work, but that is just a guess and it may be marginal. But yeah, but both options could/would work

# Methods Section

### In the Design and Analysis subsection of the Methods do we just say what we did or do we need to justify and cite why we chose certain methods?

In general your Methods section will be mostly descriptive, just stating what you did, who you ran, and how, within the full report. Caveats to that may be things like your inclusion/exclusion criteria where you may wish to provide some justification, or perhaps you want to add a citation to support any discussion of using one approach over another, e.g. using the Welch's over the Students, but if just stating we used a Pearson correlation then there is no need for citation. Overall though the Methods section should be clear and concise. 

### How exactly do we use the power and effect sizes reported in our pre-reg as part of our descriptive and inferential analyses in the Quant Report? 

There is these blogs at the end of the power chapter that might help: [https://psyteachr.github.io/quant-fun-v2/power-and-effect-sizes.html#additional-information](https://psyteachr.github.io/quant-fun-v2/power-and-effect-sizes.html#additional-information){target="_blank"} Your pre-reg is an estimate of how many people you needed. The full dataset is how many people you got. With that full dataset you can determine an effect of X. You run the analysis and find an effect of Y. If X is less than Y then your study was sufficiently powered. If X is bigger than Y then your study was underpowered. If your study is sufficiently powered and you find a significant effect then there is some support for it being a reliable effect. If your study is underpowered then no matter what you find, significant or not significant, is a bit unclear.

### Do we need subheadings in the Methods Section?

Yes. This is important as it is part of the APA formatting of a report and using the professional structure. The main subheadings would be Participants, Materials, and Procedure. You can also include a Design and Analysis subsection but sometimes this information is included in the Results section so it comes down to what works best for your writing.

### Can we change the order of subheadings in a Methods Section?

The standard is usually Participants, Materials, then Procedure as this follows the logical flow of who, with what, and how - who did you test, what did you use, and what did they do - however, if the information in your writing would make more sense in a different order then that would be acceptable. Do watch out however that logical flow is maintained in the order - do not try to explain things before the reader has the information they need to understand what you are talking about.

### Do we need an Ethics subsection in the Methods?

No but if you wanted to include one that is fine, or to include a sentence in the Procedure maybe. Please note that this is not essential for this report however. The study was granted ethics through the Ethics Committee of the School of Psychology and Neuroscience.

### What is the main difference between a Materials subsection and the Procedure subsection?

The main difference is that the Materials details what you used - stimuli, questionnaires, etc - whereas the Procedure details what participants did and how the study was run - they responded to each questionnaire shown individually with all questions presented randomly. Possible answers were...

### I have seen Cronbach's alpha mentioned in papers. What is this and do we need it?

In terms of a questionnaire, cronbach's alpha is usually meant as a measure of reliability. Normally a cronbach of $\alpha$ > .8 (so between .8 and 1) is considered very good reliability. It means that if you gave the same person the same test/questionnaire twice then they would give a similar response. You do not need to calculate this for this report as we have not covered. However you can include it if you like in the materials when talking about the MSLQ if you feel confident in it. Normally it is just a statement of "Previously this scale has been shown to have good reliability ($\alpha$ = .XX), (citation)". But this is not essential in this report as we have not covered it.

### Do we need to discuss the code we wrote and how it worked?

No. Again not wrong but it is using up space and is not needed.  If you find yourself saying "we loaded in the csv file and the filtered it for X and then mutated on a column before creating a mean value for each participant" then rephrase as "a mean value was calculated for each participant" as that is the key bit. You do not need to detail how the code work and you are likely using up words that should be saved for other sections.

### When we calculated power for the t-test in the pre-reg we were expecting equal sizes. How can I calculate the power with unequal sizes?

There is a thread on this on the TEAMS channel which you can find by searching for "Power on t-tests with different sample sizes". It was posted by Phil on the 19th May @ 10:26. However there is also an example within the the Fundamentals of Quantitative Analysis book here if that helps others: [https://psyteachr.github.io/quant-fun-v2/power-and-effect-sizes.html#uneven-groups](https://psyteachr.github.io/quant-fun-v2/power-and-effect-sizes.html#uneven-groups){target="_blank"}. Remember that you want to know the smallest `d` that you can determine with your sample sizes.

### In the participants section should I show the mean age and SD of groups regardless of whether I am looking at age or not?

It would be really recommended. In any research it is important that a reader can contextualise the sample - who were they? There is a big difference between a sample of 60 year olds and a sample of 18 year olds and if you don't tell your reader who was tested then they can never contextualise the results. Most commonly you will see a breakdown of the number of participants and the mean age and standard deviation of age for categorical variable of interest - e.g. Mature Students (N = ..., Mean Age = ..., SD = ....).  You may see other information such as nationality but that additional information is more important when it is relevant and not just standard practice.

### Should I give the total N and Mean Age etc on the sample before exclusions, after exclusions, or both?

The most important one is after exclusions because that is who will ultimately be tested upon. It might be interesting to include information prior to exclusion - as it can sometimes show issues - but that might come down to availability of words. Main thing to think about is what is relevant and to not go into too much detail of what is not relevant using up words.

### I wanted to show a mean age but some didn't give their age and I don't want to filter them out?

In that situation we might do something like "150 participants took part in our study (Mean Age = ..... years, SD = ......; 45 declined to answer)......."  At least that way you are making it clear the age and sd are based on what you have and you are making it clear that X didn't answer.

### What year and how was the data from the MSLQ collected?

In reality it was used predominantly in Level 1 (First year undergraduate class) and MSc labs, and students were asked to complete it as part of their course. They could also send the invite to others meaning that we can't guarantee that everyone was in the University of Glasgow. It was also left open on the main experiment page so others could do it as well. In terms of when it started, there is a datastamp on the datafiles that can indicate when first data was obtained.

# Results

### What happens if you are underpowered?

Have a read at this blog and see if that helps? [https://psyteachr.github.io/quant-fun-v2/power-and-effect-sizes.html#a-blog-on-interpreting-and-writing-up-power](https://psyteachr.github.io/quant-fun-v2/power-and-effect-sizes.html#a-blog-on-interpreting-and-writing-up-power){target="_blank"}. Being underpowered is just something that happens. The trick is to just avoid saying there is absolutely nothing going on here. When underpowered the truth is you just don't know what is actually happening. You know the effect is not this big but you dont know if it is really this small and can't rule it out so you have to be cautious in the write up. "The absence of evidence is not evidence of an absence" is how someone much smarter than us put it once, but it is the trap a lot of seasoned researchers fall into and one we are trying to teach people to avoid. Another way of phrasing it is, "I dont know what it is but I know what it isn't" i.e. I know it isnt a big effect but I dont know if it is a medium or small effect or nothing so I am just going to leave it at that.

### How many decimal places should we use in values?

The APA Style guide webpage has a good guide on this that it is handy to know about: [Numbers and Statistics in APA](https://apastyle.apa.org/instructional-aids/numbers-statistics-guide.pdf){target="_blank"}. This is not always adhered to though in publications and often the APA defer to the teachers of a course as the final say. However, if in doubt, using this guideline is totally fine.  One common approach people take is three decimal places for p-values and correlation values (r-values), and two decimal places for everything else. Really the hidden rule behind this approach is whether the value can go above 1 or not. p-values and correlation values (Pearson r) can't go above 1 and so we drop the first 0 and go to three decimal places - e.g. r = .015 and not r = 0.015 or r = 0.02. Often as well people might put Cohen's d to three decimal places, even though it can go above 1, especially when it is small - e.g. d = 0.005 rather than d = 0.01 as rounding up really changes the effect size. So there are always a few caveats and nuances. The main advice is to try and be consistent at this level, and do not use more than three decimal places.

### My degrees of freedom for my Welch's t-test has a decimal place. Is that ok and should I round it up?

When you run a Welch's t-test the degrees of freedom is likely to have a decimal place and should not be round up. Convention is to state it the degrees of freedom to two decimal places as it can go above 1. For example, t(95.56) = -0.91, p = 0.367, d = -0.19.

### My t-test confidence interval is showing funny? It says Inf?

That is fine if you have run a one-tailed t-test. It can be written as 95%CI = [0.61, Inf] assuming your effect is positive. If it was negative then -Inf is fine. It is because it is a one-sided test and the confidence estimate of the upper bound can't be estimated properly anymore (or that it could possibly be anything; we can't quite recall the distinction right now).

### Should figures and tables in the Results section be in APA format?

Preferably if you can but as we have not actually covered this yet it is not essential and won't be considered wrong if you do not use APA formatting.  You can get a good idea of APA formatting for figures and tables by looking at the guidelines here in the APA style blog: [https://apastyle.apa.org/style-grammar-guidelines/tables-figures](https://apastyle.apa.org/style-grammar-guidelines/tables-figures){target="_blank"}. More specifically the tabs titled "Sample Figure" and "Sample Table".

### How can I do an APA figure in R?

Again not required for this report but if you were to add `+ theme_classic()` to the end of a ggplot chain then you get something close enough. An example is below. But again this is not required in this report. 

```{r, warning=FALSE, message=FALSE}
library(tidyverse)

ggplot(iris, aes(Sepal.Length,Sepal.Width)) + 
  geom_point() +
  theme_classic()

```

### How can I do an APA table in R?

Honestly it is not worth trying to do it in R right now. Best thing is to create the table in R so you have the values and then just create a table in Word and paste the values in.  You can actually get Word to give you a template of an APA table as a starter though if that would help. If you open a new word doc, but instead of just clicking "blank document", in the search online templates box type "APA", you will see the APA template. Once you select that and it opens, scroll down to page 7 and you will see the template table. You can then copy and paste that into your doc in the Results section and then just edit that as you need.

### Can I use stars in my table to show different levels of significance? For example, p < .05 would be one *?

On the * vs ** thing. So this is used in the literature but some are a fan and some aren't. Me (Phil) personally I wouldn't use them but that is maybe a conversation for another day and doesn't really have any bearing on this report. Just flagging that at times you will get into a conversation of whether this approach is good or not so good.

### I don't have any tables in my Results section or Methods section, either for descriptives or for correlations, is that ok?

Yes. If you only compare two groups and only run one correlation, often that information can be conveyed quite effectively just using text and not using a table - keeping in mind tables count in the word count and require legends. Some will have a table because it helps them. Some will not have a table because it doesn't help them. Both are fine.

### Should I show all my assumptions figures in the Results section?

No. It isn't needed. Again it isn't wrong to do so but it is taking up space as each figure needs a legend and needs talked about.

### How many figures should be in my Results section?

Normally there is one figure per analysis. So one t-test gives you one boxplot (or equivalent) and one correlation gives you one scatterplot. If you do one t-test and one correlation in your study then you would likely have two figures in your Results section.

### Can I do more than just the minimum analysis of one t-test and one correlation?

Ultimately we will mark and give feedback on what you submit. That means that if you were inspired to run a series of correlations then we would still mark that. However, please note that this report is not about showing how many analyses you can do. It is about showing how you can write up the two research analyses. The more analyses you do, the more space you take up from writing your introduction and discussion, and remember that every finding would need discussed in the Discussion. So whilst you can do more analyses, it is not advised.

### Is it possible to compare different effect sizes such as Pearson's r, Cohen's d, partial eta-squared, and Hedges' g?

Yes but the best thing to do is convert them all into the same effect size first so that you are comparing like for like. Often if you google "convert Cohen's d to...." you might find what you need and then you can compare across studies. If you can't convert the given effect size into a different one for some reason - not enough information for example - then you can compare them semantically "small vs large" but it would be worth acknowledging that they were different effect sizes in your writing somehow.

### My data looks normal but the Shapiro-Wilks is showing as significant which would suggest not normal. Can I ignore my Shapiro-Wilks?

This is likely because you have a large sample size. Here we would advise disregarding the Shapiro-Wilks but stating why. This paper [https://link.springer.com/article/10.3758/s13428-021-01587-5](https://link.springer.com/article/10.3758/s13428-021-01587-5){target="_blank"} has a useful point in it: "Formal tests for normality have been criticized because they have low power at small sample sizes and almost always yield significant deviations from normality at large sample sizes (Ghasemi & Zahediasl, 2012)."

### Do we need to justify and give citations for each action and analysis in the Methods and Results?

This aspect can be more brief in the full report as compared to the pre-registration so aim to provide brief justifications where needed. You would not need to citation the tests themselves e.g. a citation for a Pearson's correlation is not needed. Citations are more for when you are stating a reason for doing something that is perhaps a little different from the current norm - such as "We are using Welch's as it is better at controlling False Positive (citation)" as opposed to "We are using Pearsons because all assumptions were met".

### My test was significant but the effect size/relationship was really really small. Have I done something wrong?

Probably not. When you have a large sample, small effects/relationships will be significant. Check your code and check your degrees of freedom make sense. If all that seems correct then your result is probably correct just that you have a very significant and very small effect. This is why we should be more interested in effect sizes than p-values.

### You said something about checking degrees of freedom to check my results make sense. What does that mean?

When working with big data - or any data really - it can be easy to make a mistake. One thing you do know though is how to calculate the degrees of freedom from your test. In a correlation it is N - 2 for example. So if you have 100 participants then your degrees of freedom should be 98. If you run a correlation with 100 people and the degrees of freedom (parameter) is not 98, something is wrong.  Likewise with a t-test but it is a bit harder because you are likely to run a Welch's t-test so the degrees of freedom is harder to calculate. However, if you briefly run it as a Student's between-subjects t-test (by setting `var.equal = TRUE`), check the degrees of freedom makes sense, then switch it back to a Welch's t-test, you can be pretty sure your Welch's output is correct. If the degrees of freedom didn't make sense as a Student's test then something is wrong.

# Citing R

### Should we cite R, RStudio and packages used?

The minimum should be citing R and packages used. There is a thread on the TEAMS channel which can be reached by searching the channel for "How to cite R and packages". It was posted by Phil on 1st June 16:40pm.

### How do you cite R and packages?

Information on this can be found on a thread on the TEAMS channel which can be reached by searching the channel for "How to cite R and packages". It was posted by Phil on 1st June 16:40pm. The main thing to say is that it can be tricky to know what to put as a reference for the individual packages and we understand that. We would suggest having the last name of author, first initial of author, year, and package name if possible.

### Do we need to cite all packages contained within Tidyverse?

There is debate on this as mentioned on a thread on the TEAMS channel which can be reached by searching the channel for "How to cite R and packages". It was posted by Phil on 1st June 16:40pm. We would recommend that for this report it is fine to just cite the tidyverse.